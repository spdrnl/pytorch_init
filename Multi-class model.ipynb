{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as trans\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dsets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=trans.ToTensor(), \n",
    "    download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images 60000\n",
      "Type <class 'torch.Tensor'>\n",
      "size of images torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print('Number of images {}'.format(len(train_set)))\n",
    "print('Type {}'.format(type(train_set[0][0])))\n",
    "print('size of images {}'.format(train_set[0][0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticModel(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(MultiLogisticModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 28*28\n",
    "out_dim = 10\n",
    "model = MultiLogisticModel(in_dim,out_dim).cuda()\n",
    "model.to(device)\n",
    "\n",
    "lr = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda x: x*0.7)\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batchSize,\n",
    "    num_workers = 12,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 923.2346438169479\n",
      "epoch 1, loss 923.2074882984161\n",
      "epoch 2, loss 923.2074898481369\n",
      "epoch 3, loss 923.2165929079056\n",
      "epoch 4, loss 923.2242610454559\n",
      "epoch 5, loss 923.2310808897018\n",
      "epoch 6, loss 923.203705906868\n",
      "epoch 7, loss 923.2037031650543\n",
      "epoch 8, loss 923.2124952077866\n",
      "epoch 9, loss 923.2196451425552\n",
      "epoch 10, loss 923.2262853384018\n",
      "epoch 11, loss 923.1990579366684\n",
      "epoch 12, loss 923.1990647315979\n",
      "epoch 13, loss 923.2079836130142\n",
      "epoch 14, loss 923.215115070343\n",
      "epoch 15, loss 923.2220945358276\n",
      "epoch 16, loss 923.195160150528\n",
      "epoch 17, loss 923.195161819458\n",
      "epoch 18, loss 923.2037670612335\n",
      "epoch 19, loss 923.209562420845\n",
      "epoch 20, loss 923.2184315919876\n",
      "epoch 21, loss 923.1903346776962\n",
      "epoch 22, loss 923.1903326511383\n",
      "epoch 23, loss 923.1992877721786\n",
      "epoch 24, loss 923.2060656547546\n",
      "epoch 25, loss 923.2117710113525\n",
      "epoch 26, loss 923.1871325969696\n",
      "epoch 27, loss 923.1871324777603\n",
      "epoch 28, loss 923.1954009532928\n",
      "epoch 29, loss 923.2022411823273\n",
      "epoch 30, loss 923.2091890573502\n",
      "epoch 31, loss 923.181892991066\n",
      "epoch 32, loss 923.1818881034851\n",
      "epoch 33, loss 923.1908762454987\n",
      "epoch 34, loss 923.1972444057465\n",
      "epoch 35, loss 923.2026755809784\n",
      "epoch 36, loss 923.1808882951736\n",
      "epoch 37, loss 923.1808930635452\n",
      "epoch 38, loss 923.188227057457\n",
      "epoch 39, loss 923.1941462755203\n",
      "epoch 40, loss 923.2001214027405\n",
      "epoch 41, loss 923.1734992265701\n",
      "epoch 42, loss 923.1735044717789\n",
      "epoch 43, loss 923.1821032762527\n",
      "epoch 44, loss 923.1889672279358\n",
      "epoch 45, loss 923.1957176923752\n",
      "epoch 46, loss 923.1685814857483\n",
      "epoch 47, loss 923.1685863733292\n",
      "epoch 48, loss 923.177531838417\n",
      "epoch 49, loss 923.1834617853165\n",
      "epoch 50, loss 923.1916373968124\n",
      "epoch 51, loss 923.1656435728073\n",
      "epoch 52, loss 923.1656444072723\n",
      "epoch 53, loss 923.1739325523376\n",
      "epoch 54, loss 923.1805791854858\n",
      "epoch 55, loss 923.1881436109543\n",
      "epoch 56, loss 923.1599850654602\n",
      "epoch 57, loss 923.1599879264832\n",
      "epoch 58, loss 923.1690571308136\n",
      "epoch 59, loss 923.1767371892929\n",
      "epoch 60, loss 923.1828354597092\n",
      "epoch 61, loss 923.1565843820572\n",
      "epoch 62, loss 923.1565917730331\n",
      "epoch 63, loss 923.1651200056076\n",
      "epoch 64, loss 923.1718487739563\n",
      "epoch 65, loss 923.17895591259\n",
      "epoch 66, loss 923.1518158912659\n",
      "epoch 67, loss 923.1518065929413\n",
      "epoch 68, loss 923.1602498292923\n",
      "epoch 69, loss 923.1679373979568\n",
      "epoch 70, loss 923.1737993955612\n",
      "epoch 71, loss 923.147594332695\n",
      "epoch 72, loss 923.1475915908813\n",
      "epoch 73, loss 923.1563259363174\n",
      "epoch 74, loss 923.163824558258\n",
      "epoch 75, loss 923.1691914796829\n",
      "epoch 76, loss 923.1432920694351\n",
      "epoch 77, loss 923.1433020830154\n",
      "epoch 78, loss 923.1520270109177\n",
      "epoch 79, loss 923.1578950881958\n",
      "epoch 80, loss 923.1653959751129\n",
      "epoch 81, loss 923.1403249502182\n",
      "epoch 82, loss 923.1403152942657\n",
      "epoch 83, loss 923.148487329483\n",
      "epoch 84, loss 923.1542909145355\n",
      "epoch 85, loss 923.1600041389465\n",
      "epoch 86, loss 923.1359798908234\n",
      "epoch 87, loss 923.1359791755676\n",
      "epoch 88, loss 923.1442844867706\n",
      "epoch 89, loss 923.1506296396255\n",
      "epoch 90, loss 923.1582283973694\n",
      "epoch 91, loss 923.1302427053452\n",
      "epoch 92, loss 923.1302423477173\n",
      "epoch 93, loss 923.1388139724731\n",
      "epoch 94, loss 923.1462671756744\n",
      "epoch 95, loss 923.1528375148773\n",
      "epoch 96, loss 923.1264845132828\n",
      "epoch 97, loss 923.1264854669571\n",
      "epoch 98, loss 923.1349848508835\n",
      "epoch 99, loss 923.1423624753952\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    runningLoss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(images).to(device)\n",
    "        loss = loss_function(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        runningLoss += loss.item()\n",
    "    print('epoch {}, loss {}'.format(epoch, runningLoss))\n",
    "    lr_scheduler.step()\n",
    "    if epoch%5 == 0:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda x: x*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def successRate(predicted, labels):\n",
    "    predict = [np.argmax(p.detach().numpy()) for p in predicted]\n",
    "    actual =  [labels[i].item() for i in range(len(predicted))]\n",
    "    correct = [i for i, j in zip(predict, actual) if i == j]\n",
    "    return(len(correct)/(len(predict)))\n",
    "\n",
    "predicted = model.forward(images) \n",
    "predicted.size()\n",
    "successRate(predicted, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet = dsets.MNIST(root='./data', train = False, transform=trans.ToTensor(), download=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testSet,  batch_size = 10000, shuffle = True)\n",
    "\n",
    "testData = iter(testloader)\n",
    "images, labels = testData.next()\n",
    "output = model.forward(images.view(-1, 28 * 28))\n",
    "successRate(output,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multiclassmodel.pkl')\n",
    "model.load_state_dict(torch.load('multiclassmodel.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
